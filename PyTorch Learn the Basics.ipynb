{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
    "\n",
    "An introduction to PyTorch taken from the pytorch beginner tutorials.\n",
    "\n",
    "Using the FashionMNIST dataset to train a classification NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "Tensors are specil data structures similar to arrays and matrices (they're basically N-dimensional matrices). Pytorch uses these to encode input/outputs and model params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly from data\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From numpy\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.1204, 0.5206],\n",
      "        [0.2104, 0.0039]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From another tensor\n",
    "x_ones = torch.ones_like(x_data)  # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "# overrides the datatype of x_data\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.8940, 0.3847, 0.0450],\n",
      "        [0.0967, 0.9241, 0.6508]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# With random or constant values\n",
    "shape = (2, 3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atrributes of a Tensor\n",
    "These describe the tensor's shape, dtype and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Tensors\n",
    "All tensor ops can be run on the GPU. By default tensors are created on the CPU and must be explicitly moved to the GPU using the `.to` method. This can be time/memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Standard NP indexing/slicing\n",
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ', tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Joining tensors\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arithmetic Operations\n",
    "# This computes the matrix multiplication between two tensors.\n",
    "# y1, y2, y3 will have the same value\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product.\n",
    "# z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# Single element tensors\n",
    "# using .item() converts a single element tensor to a python number\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# In-place opes store result in place and are denoted by a `_` suffix\n",
    "# They sevae some memory but can cause issues with derivatives, their use is discouraged.\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge with NumPy\n",
    "Tensors on the CPU and NumPy arrays can share their memory location, changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "Code for processing data samples can get messy and hard to maintain (even with these tools). However we want to decouple training code from datasets handling.\n",
    "\n",
    "Dataloaders and Datasets allow us to use preloaded datasets and our own data. *Dataset* stores the samples and their labels, *Dataloader* wraps an iterable around the *dataset* to enable easy access to samples.\n",
    "\n",
    "Pytorch provides a number of datasets that are preloaded and can be used to prototype and benchmark models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a dataset\n",
    "We will work with the [Fashin-MNIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist/) dataset from TorchVision.\n",
    "- num_train_samples = 60k\n",
    "- num_test_samples = 10k\n",
    "- num_classes = 10\n",
    "- image_shape = (1,28,28) i.e. 28x28 grayscale\n",
    "\n",
    "We use the following parameters to load the dataset:\n",
    "- `root` is the path to the data\n",
    "- `train` specifics the train/test set\n",
    "- `download=true` downloads the data if not available at root\n",
    "- `transform` and `target_transform` specify the feature and label transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating and Visualising the Dataset\n",
    "Datasets can be indexed manually like a list: `training_data[index]` and visualised using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols*rows+1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom dataset for your files\n",
    "A custom Dataset class must implement the ```__init__, __len__ and __getitem__``` methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\_\\_init\\_\\_\n",
    "Runs on instantiation. We assume that the labels are stored in a csv separate to the images.\n",
    "\n",
    "#### \\_\\_len\\_\\_ \n",
    "Returns the number of samples in the dataset.\n",
    "\n",
    "#### \\_\\_getitem\\_\\_\n",
    "Loads and returns a sample from the dataset at thek given index ```idx```. Based on the index it converts an image to a tensor (from disk) and gets the label from the csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        \"\"\"Loads the csv into memory but not the images\n",
    "\n",
    "        Args:\n",
    "            annotations_file ([type]): [description]\n",
    "            img_dir ([type]): [description]\n",
    "            transform ([type], optional): [description]. Defaults to None.\n",
    "            target_transform ([type], optional): [description]. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Loads and returns a sample from the dataset at thek given index ```idx```. \n",
    "        Based on the index it converts an image to a tensor (from disk) and gets the label from the csv data.\n",
    "\n",
    "        Args:\n",
    "            idx (int): image idx\n",
    "\n",
    "        Returns:\n",
    "            (image,label,): [description]\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing your data for training with Dataloaders\n",
    "The `Dataset` retrieves our datasets features and labels one sample at a time.\n",
    "We typically want to use minibatches and reshuffle data at every epoch (maybe) to reduce overfitting. We can also use `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "`Dataloader` is an iterable that abstracts all this away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the DataLoader\n",
    "Once the dataset is loaded in the dataloader we can iterate over it as needed.\n",
    "Each iterations returns a batch of `train_features` and `train_labels` (of batch size=64 each). The data is shuffled after each epoch (all the batches of data have been seen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: Coat(4)\n"
     ]
    }
   ],
   "source": [
    "# Display the image and label\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {labels_map[label.item()]}({label})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "Data sometimes needs to be **transformed**, manipulated to make it suitable for training.\n",
    "\n",
    "All TorchVision datasets have 2 paramters, `transform` to modify features and `target_transform` to modify labels, that accept callables. Several examples are available in  `torchvision.transforms`.\n",
    "\n",
    "FashionMNIST features are in PIL image format, labels are integers. Fro training we need the features as normalized tensors and the labels as one-hot encoded tensors. To make these transformations we us `ToTensor` and `Lambda`.\n",
    "\n",
    "### ToTensor()\n",
    "Converts a PIL image/NumPy array to `FloatTensor` and scales the pixel intensities to [0,1]\n",
    "\n",
    "### Lambda Transforms\n",
    "Applies a user defined lambda. We will define a function the takes an int and converts it to a one hot encoded tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(\n",
    "        10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "NNs comprise layers/modules that operate on the data. Everything needed is in **torch.nn**. Every module in PyTorch subclasses **nn.Module**. A NN is a module that consists of other modules (layers). This allows for building and managing complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the class\n",
    "Subclass `nn.Module`, and initialise the NN layers in `__init__`. Every `nn.Module`  subclass implements the operations on the input data in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Layers\n",
    "Lets see what happens to a sample minibatch of 3 28x28 images as it passes through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Flatten \n",
    "We initialize the flatten layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Linear\n",
    "The linear layer applieas a linear transform on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.ReLU\n",
    "Non-linear activations create the complex mappings between the model input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.1253, -0.3219,  0.3639,  0.0467, -0.1325, -0.1110, -0.5925,  0.5686,\n",
      "         -0.5764,  0.0035, -0.2198, -0.7203, -0.6499, -0.0496, -0.5509,  0.1742,\n",
      "          0.3919, -0.2112,  0.4511, -0.3401],\n",
      "        [ 0.0866, -0.4604,  0.4971, -0.2294,  0.1781, -0.0484, -0.7706,  0.7902,\n",
      "         -0.2367, -0.1641, -0.6644, -0.5860, -0.5936, -0.1562, -0.6433, -0.0020,\n",
      "          0.3931,  0.1530,  0.2274,  0.0027],\n",
      "        [-0.0481, -0.3700,  0.3255, -0.0960,  0.1798, -0.0046, -0.7114,  0.4499,\n",
      "         -0.3260, -0.0264, -0.5448, -0.7023, -0.3717,  0.0079, -0.3609,  0.4968,\n",
      "          0.2307,  0.1809, -0.0315, -0.1671]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1253, 0.0000, 0.3639, 0.0467, 0.0000, 0.0000, 0.0000, 0.5686, 0.0000,\n",
      "         0.0035, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1742, 0.3919, 0.0000,\n",
      "         0.4511, 0.0000],\n",
      "        [0.0866, 0.0000, 0.4971, 0.0000, 0.1781, 0.0000, 0.0000, 0.7902, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3931, 0.1530,\n",
      "         0.2274, 0.0027],\n",
      "        [0.0000, 0.0000, 0.3255, 0.0000, 0.1798, 0.0000, 0.0000, 0.4499, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0079, 0.0000, 0.4968, 0.2307, 0.1809,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Sequential\n",
    "This is an ordered container of modules that passes data through automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Softmax\n",
    "The last linear layer returns the logits which are passed through Softmax to rescale the logits to [0,1]. We interpret this as probabilities (not really onfidences though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "Many layers inside the network are *parametrized*, i.e. have weights and biases to be optimized. Subclassing `nn.Module` automatically tracks all the fields defined and makes the parameters accessible using `parameters()` or `named_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0296,  0.0350, -0.0141,  ...,  0.0035,  0.0116, -0.0268],\n",
      "        [ 0.0356,  0.0252,  0.0141,  ..., -0.0260,  0.0170,  0.0119]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0015, -0.0204], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0095,  0.0147, -0.0163,  ...,  0.0387,  0.0070, -0.0214],\n",
      "        [ 0.0304,  0.0146, -0.0108,  ..., -0.0253,  0.0180,  0.0066]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0251, -0.0138], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0212, -0.0408, -0.0232,  ...,  0.0363,  0.0100,  0.0247],\n",
      "        [ 0.0190,  0.0387, -0.0143,  ..., -0.0162, -0.0267, -0.0057]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0033, -0.0272], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each parameter and print its size and a preview of the values\n",
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation with `torch.autograd`\n",
    "The most common training algoirthm is **back propagation**. This adjust model weights accoridng to the **gradient** of the loss function withr espect to the given parameter/weight.\n",
    "\n",
    "Consider the simplest one layer network with input `x` and parameters `w` nd `b` and a loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors, Functions and the Computational Graph\n",
    "The code defines the following **computational graph**:\n",
    "![Computational Graph](https://pytorch.org/tutorials/_images/comp-graph.png)\n",
    "\n",
    "In this network, `w` and `b` are the **parameters** to be optimized. We need to compute the *gradients* of the loss function w.r.t. those varaibles (`requires_atuograd=True`)\n",
    "\n",
    "A function that we apply to tensors to construct the computational graph is in fact an object of class `Function` that can compute the function in the *forward* direction and it's derivating during the 1backward propagation` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7ff17a5231f0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7ff178bfd6a0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =', z.grad_fn)\n",
    "print('Gradient function for loss =', loss.grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients\n",
    "To optimize the wieghts in the entwork we compute the derivatives of the loss function w.r.t the parameters by calling `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad` (not done manually by us).\n",
    "\n",
    "### Disabling Gradient Tracking\n",
    "We can turn off gradient tracking to improve efficiency during inference (only forward pass needed), or to **freeze parameters** for [finetuning](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html). We still register all the parameters/weights with the optimizer as normal however.\n",
    "\n",
    "We can do this by surrounding the code in a `with torch.no_grad()` block or by calling `detach()` on the tensor.\n",
    "\n",
    "### More on Computational Graphs\n",
    "Because the DAG for the computational graph is created from scratch after each `.backward()` call, this allows us to use control flow statements in the model and change the shape, size and operations at each iteration if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Model Parameters\n",
    "Now that we have a model and data its time to train, validate and test our model by optimizing its parameters on our data. \n",
    "\n",
    "In each iteration/epoch the model makes a prediction, calculates the error/loss, collects the derivatives and **optimizes** these using gradient descent. See [backpropoagation by 3B1B](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Hparams are adjustable values that lat us control the optimization process ([read more](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)).\n",
    "\n",
    "We define the following hparams:\n",
    "- **Number of epochs** - how many times to iterate of the dataset\n",
    "- **Batch size** - number of data samples propagated before parameters are updates\n",
    "- **Learning rate** - smaller values train slower but have more predictable behaviour during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Loop\n",
    "Once we set the Hparams we must trian and optimize the model (looping over epochs).\n",
    "Each epoch has a **training loop** and a **validation/testing loop**.\n",
    "\n",
    "#### Loss Function\n",
    "Measures the how incorrect the netowrk result is compared to the target value, we want to **minimize** this.\n",
    "\n",
    "Common loss functions are `nn.MSELoss` (Mean Square Error) for regression, and `nn.NLLLoss`(Negative Log Likelihood) for classification. `nn.CrossEntropyLoss` combines `nn.LogSoftmax` and `nn.NLLLoss`.\n",
    "\n",
    "We pass our models logits.output to `nn.CrossEntropyLoss`, which normalizes the logits and computes the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "We will use Stochastic Gradient Descent. the `optimizer` object encapsulated the optimization logic.\n",
    "\n",
    "Inside the trianing loop optimization happens in 3 steps:\n",
    "1. Call `optimizer.zero_grad()` to reset the gradients of the model parameters. Gradients add up by default (useful for if we want a bigger minibatch but can't do it in one go because of memory constraints).\n",
    "2. Backpropagate the loss by calling `loss.backword()`.\n",
    "3. Call `optimizer.step()` to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    \"\"\"Loops over our optimization code\"\"\"\n",
    "    # The actual size of the dataset, len(dataloader) gives the number of batches returned\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.to(device))\n",
    "        loss = loss_fn(pred.cpu(), y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    \"\"\"Evaluates the model's performance against the test data\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.to(device))\n",
    "            pred = pred.cpu()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "-------------------\n",
      "loss: 2.311255  [    0/60000]\n",
      "loss: 2.292602  [ 6400/60000]\n",
      "loss: 2.275878  [12800/60000]\n",
      "loss: 2.264900  [19200/60000]\n",
      "loss: 2.252029  [25600/60000]\n",
      "loss: 2.234711  [32000/60000]\n",
      "loss: 2.240597  [38400/60000]\n",
      "loss: 2.210972  [44800/60000]\n",
      "loss: 2.202068  [51200/60000]\n",
      "loss: 2.176918  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 36.1%, Avg loss: 2.167159 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Initialise the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} \\n-------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    test_loop(test_dataloader, model, loss_fn, device)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model\n",
    "PyTorch models store the learned parameters in an internal `state_dict` which can be persisted using `torch.save()`.\n",
    "\n",
    "To load model weights you need an instance of the model first and then use `load_state_dict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "model = models.vgg16() # We don't specify pretrained=True, i.e. don't load default weights\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval() # Important to do this before inference as it sets the dropout and abtchnorm layers to eval mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Models with Shapes\n",
    "When loading model weights, we needed to first instantiate the model class because it defines the structure of a network. **DO NOT DO THIS** can save the structure together with the model by passing `model` instead of `model.state_dict()` to the save function. However, this requires the class definition (the actual python file) to be in the same location relative to everything else. It's a bad idea"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30b2d88519b42e2b1be5e12579ff4b114f9834f469b31be7f82137c9c3522cd2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Testing': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
