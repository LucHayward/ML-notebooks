{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch profiler\n",
    "We will setup a simple resnet model and use it to explore the profiler.\n",
    "\n",
    "## Steps\n",
    "1. Import all necessary libraries\n",
    "2. Instantiate a simple Resnet model\n",
    "3. Using profiler to analyze execution time\n",
    "4. Using profiler to analyze memory consumption\n",
    "5. Using tracing functionality\n",
    "6. Examining stack traces\n",
    "7. Visualizing data as a flamegraph\n",
    "8. Using profiler to analyze long-running jobs\n",
    "\n",
    "###  1,2. Import libraries and instantiate resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "model = models.resnet18()\n",
    "inputs = torch.randn(5,3,224,244)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the profiles to analyze execution time\n",
    "PyTorch profiler is enabled through the context manager and accepts various parameters such as:\n",
    "- activities\n",
    "  - ProfilerActivity.CPU\n",
    "  - ProfilerActivity.CUDA\n",
    "- record shapes - record shapes of operator inputs\n",
    "- profile memory - report memory consumed by model's tensors\n",
    "- use_cuda - measure execution time of CUDA kernels\n",
    "\n",
    "Note that we can use `record_function` context manager to label abritrary code ranges with user provided names. `model_inference` is the label below.\n",
    "\n",
    "Profiler allows us to check which operators were called during executioni of a code range wrapped with a profiler context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         2.71%       5.570ms        98.72%     202.889ms     202.889ms             1  \n",
      "                     aten::conv2d         0.08%     170.000us        64.62%     132.821ms       6.641ms            20  \n",
      "                aten::convolution         0.09%     178.000us        64.54%     132.651ms       6.633ms            20  \n",
      "               aten::_convolution         0.15%     309.000us        64.45%     132.473ms       6.624ms            20  \n",
      "         aten::mkldnn_convolution        64.17%     131.884ms        64.30%     132.164ms       6.608ms            20  \n",
      "                 aten::batch_norm         0.06%     133.000us        22.66%      46.583ms       2.329ms            20  \n",
      "     aten::_batch_norm_impl_index         0.16%     320.000us        22.60%      46.450ms       2.322ms            20  \n",
      "          aten::native_batch_norm        17.28%      35.512ms        22.43%      46.099ms       2.305ms            20  \n",
      "                 aten::max_pool2d         0.09%     193.000us         5.34%      10.977ms      10.977ms             1  \n",
      "    aten::max_pool2d_with_indices         5.25%      10.784ms         5.25%      10.784ms      10.784ms             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 205.529ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a finer granularity we can pass `group_by_input_shape=True` (this requres running the profiler with `record_shapes=True` too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                  model_inference         2.71%       5.570ms        98.72%     202.889ms     202.889ms             1                                                                                []  \n",
      "                     aten::conv2d         0.02%      32.000us        15.52%      31.889ms       7.972ms             4                             [[5, 64, 56, 61], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.02%      39.000us        15.50%      31.857ms       7.964ms             4                     [[5, 64, 56, 61], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.03%      57.000us        15.48%      31.818ms       7.955ms             4     [[5, 64, 56, 61], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        15.41%      31.682ms        15.45%      31.761ms       7.940ms             4                             [[5, 64, 56, 61], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.01%      24.000us        12.74%      26.189ms       8.730ms             3                          [[5, 128, 28, 31], [128, 128, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.01%      26.000us        12.73%      26.165ms       8.722ms             3                  [[5, 128, 28, 31], [128, 128, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.04%      75.000us        12.72%      26.139ms       8.713ms             3  [[5, 128, 28, 31], [128, 128, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        12.65%      26.007ms        12.68%      26.064ms       8.688ms             3                          [[5, 128, 28, 31], [128, 128, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.01%      22.000us        10.16%      20.875ms       6.958ms             3                            [[5, 512, 7, 8], [512, 512, 3, 3], [], [], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 205.529ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the rpofiler to analyse GPU based performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.66%       2.617ms        99.99%     396.822ms     396.822ms       0.000us         0.00%       7.691ms       7.691ms             1  \n",
      "                                           aten::conv2d         0.03%     120.000us        96.88%     384.472ms      19.224ms       0.000us         0.00%       5.861ms     293.050us            20  \n",
      "                                      aten::convolution         0.04%     155.000us        96.85%     384.352ms      19.218ms       0.000us         0.00%       5.861ms     293.050us            20  \n",
      "                                     aten::_convolution         0.06%     219.000us        96.81%     384.197ms      19.210ms       0.000us         0.00%       5.861ms     293.050us            20  \n",
      "                                aten::cudnn_convolution        11.27%      44.733ms        96.75%     383.978ms      19.199ms       5.861ms        76.21%       5.861ms     293.050us            20  \n",
      "void explicit_convolve_sgemm<float, int, 1024, 5, 5,...         0.00%       0.000us         0.00%       0.000us       0.000us       1.447ms        18.81%       1.447ms     482.333us             3  \n",
      "                 maxwell_scudnn_128x64_relu_small_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us       1.306ms        16.98%       1.306ms     261.200us             5  \n",
      "                                       aten::batch_norm         0.03%     118.000us         1.98%       7.839ms     391.950us       0.000us         0.00%     921.000us      46.050us            20  \n",
      "                           aten::_batch_norm_impl_index         0.05%     193.000us         1.95%       7.721ms     386.050us       0.000us         0.00%     921.000us      46.050us            20  \n",
      "                                 aten::cudnn_batch_norm         1.35%       5.342ms         1.90%       7.528ms     376.400us     921.000us        11.98%     921.000us      46.050us            20  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 396.868ms\n",
      "Self CUDA time total: 7.691ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18().cuda()\n",
    "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More information\n",
    "More information on profiling memory, stack traces and more can be found in the [tutorial](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-profiler-to-analyze-memory-consumption)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30b2d88519b42e2b1be5e12579ff4b114f9834f469b31be7f82137c9c3522cd2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('Testing': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
